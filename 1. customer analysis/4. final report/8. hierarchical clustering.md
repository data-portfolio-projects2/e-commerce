![image](https://github.com/user-attachments/assets/9d6d58a1-2f78-47a4-910a-266d7ba55d2a)

 ```     
       V1          
 Min.   :-5.11879  
 1st Qu.:-0.64543  
 Median : 0.06652  
 Mean   : 0.00000  
 3rd Qu.: 0.76029  
 Max.   : 1.44614 
 ```

## Applying Euclidean, Manhattan & Cosine Distance Methods

![image](https://github.com/user-attachments/assets/9f59b1f3-84b3-4031-97d5-98d8893ee726)

## Evaluation

![image](https://github.com/user-attachments/assets/1495e1f8-256d-4fd6-926d-9a0d64cece94)

## Silhouette Plots for Clustering Results

The image shows silhouette plots for clustering results using three different distance metrics: Euclidean, Manhattan, and Cosine.  For each metric, there are 4 clusters identified (C1, C2, C3, C4). The total number of data points is 5843 (n = 5843).

**Each subplot shows:**

*   **`j`**: The cluster number (1, 2, 3, or 4).
*   **`n_j`**: The number of data points in that cluster.
*  **`ave_iâˆˆCj S_i`**: The average silhouette width for the points in that cluster.

**Silhouette Plot (Euclidean)**

*   **Cluster 1:** `n_1 = 1705`, `S_i = -0.05`
*   **Cluster 2:** `n_2 = 1014`, `S_i = -0.03`
*   **Cluster 3:** `n_3 = 2964`, `S_i = -0.03`
*   **Cluster 4:** `n_4 = 160`, `S_i = 0.68`
*   **Average silhouette width:** -0.02

**Silhouette Plot (Manhattan)**

*   **Cluster 1:** `n_1 = 2117`, `S_i = -0.02`
*   **Cluster 2:** `n_2 = 2187`, `S_i = -0.17`
*   **Cluster 3:** `n_3 = 456`, `S_i = -0.01`
*   **Cluster 4:** `n_4 = 1083`, `S_i = 0.005`
*   **Average silhouette width:** -0.07

**Silhouette Plot (Cosine)**

*   **Cluster 1:** `n_1 = 2118`, `S_i = 0.18`
*   **Cluster 2:** `n_2 = 1670`, `S_i = 0.16`
*   **Cluster 3:** `n_3 = 851`, `S_i = -0.19`
*   **Cluster 4:** `n_4 = 1204`, `S_i = -0.19`
*   **Average silhouette width:** 0.05

**Analysis and Insights**

*   **Silhouette Width:** Silhouette width is a measure of how similar an object is to its own cluster compared to other clusters. The values range from -1 to +1, where:
    *   Values near +1 indicate that an object is well-clustered.
    *   Values near 0 indicate that an object is close to a decision boundary between two clusters.
    *   Values near -1 indicate that an object might be assigned to the wrong cluster.

* **Euclidean Distance:** The average silhouette width is negative (-0.02), which indicates that the data points don't fit neatly into clusters.  Clusters 1, 2 and 3 have poor results. The clustering appears to be driven by the small Cluster 4 that is well-separated.
* **Manhattan Distance:** The Manhattan metric has the lowest overall score (avg. -0.07). Many negative silhouette values indicate that the data points are very poorly assigned to these 4 clusters.
*  **Cosine Distance:** Cosine distance provides the best of the three, as indicated by the average silhouette width, which is 0.05. Clusters 1 and 2 have positive values and appear to be well-defined.  Clusters 3 and 4 are poorly defined with negative values.

*   **Cluster Sizes:**  The cluster sizes vary widely, especially for the Euclidean distance. The smallest cluster with Euclidean distance contains only 160 points, while Cluster 3 has 2964 points.

*   **Choice of Distance Metric:** It appears that the cosine distance might be most appropriate compared to the Euclidean and Manhattan distances, since it produces the highest overall average silhouette width and shows at least 2 well-separated clusters. However, even with the cosine distance, there are two clusters that exhibit negative silhouette scores.

*   **Number of Clusters:** These results suggest that 4 clusters might not be optimal for the underlying dataset, at least not with the parameters used. The low silhouette scores, and even negative average scores for the Euclidian and Manhattan distances, indicate a need to either:
    *  Evaluate with more or less clusters.
    * Change the clustering parameters.
    * Improve feature engineering.

In summary, the silhouette plots provide evidence that the clusters obtained using Euclidean and Manhattan distances are not good. The clusters obtained using cosine distance shows better results, but the results are still not optimal. Further investigation into different clustering parameters, distance metrics, or preprocessing techniques might be required to achieve better clustering results.

## Cluster Evaluation Metrics: Dunn Index & Davies-Bouldin Index

```
Dunn Index (Euclidean): 0 

Dunn Index (Manhattan): 0 

Dunn Index (Cosine): -3.925433e-10
```

### 1. Dunn Index

**Euclidean:** 0
**Manhattan:** 0
**Cosine:** -3.925433e-10

**Interpretation:**

The Dunn Index measures the separation between clusters. A higher Dunn index indicates better separation, while a lower value suggests poor separation. A value of 0 or close to 0 indicates poor cluster separation.

In your case, all three distance metrics show very low or zero Dunn index values, which suggests that the clusters are poorly separated, meaning the clustering algorithms did not effectively divide the data into well-separated groups.

For the Cosine method, the negative value is quite unusual and indicates a potential issue with the clustering or distance computation.

```
Davies-Bouldin Index (Euclidean): 11.72764 

Davies-Bouldin Index (Manhattan): 89.07342 

Davies-Bouldin Index (Cosine): 12.62174
```

### 2. Davies-Bouldin Index

**Euclidean:** 11.72764
**Manhattan:** 89.07342
**Cosine:** 12.62174

**Interpretation:**

The Davies-Bouldin Index (DBI) evaluates cluster cohesion and separation. A lower DBI indicates better clustering, with well-separated and compact clusters.

Euclidean (11.73) and Cosine (12.62) have relatively low DBI values compared to Manhattan (89.07). This suggests that the Euclidean and Cosine clusters are better separated and more compact than those formed using the Manhattan distance.

Manhattan shows a very high DBI value, indicating poor clustering, where clusters are not compact, and the separation between clusters is weak.

### Summary of the Results:

*   **Cluster Separation:** The Dunn Index values of 0 across all distance metrics indicate poor separation between the clusters.
*   **Cluster Cohesion:** The Davies-Bouldin Index for Euclidean and Cosine distances suggests slightly better cohesion and separation than Manhattan, with the latter yielding a very high value.
*   **Overall Clustering Performance:** Based on these indices, your clustering results do not show strong performance in terms of clearly defined clusters. The clusters may need to be adjusted, either by exploring different distance measures or reconsidering the number of clusters.

### Potential Actions:

*   **Reevaluate Clustering Methodology:** You may want to try using different clustering techniques (like K-means, DBSCAN, or Gaussian Mixture Models) to see if the results improve.
*   **Tune Parameters:** Try varying the number of clusters (k) to see if a different value provides better-defined clusters.
*   **Data Scaling:** Ensure your data is properly scaled, especially for distance-based methods like Euclidean and Manhattan.
*   **Check for Outliers:** Outliers could impact the clustering results, so it's worth checking and possibly removing outliers to see if the clusters improve.
